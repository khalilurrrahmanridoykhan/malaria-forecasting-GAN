{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff74fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915faba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predictions for pv_rate\n",
      "Predictions count: 71\n",
      "Predictions for pv_rate saved to ./predictions_pv_rate.csv\n",
      "Starting predictions for pf_rate\n",
      "Predictions count: 71\n",
      "Predictions for pv_rate saved to ./predictions_pv_rate.csv\n",
      "Starting predictions for pf_rate\n",
      "Predictions count: 71\n",
      "Predictions for pf_rate saved to ./predictions_pf_rate.csv\n",
      "Starting predictions for mixed_rate\n",
      "Predictions count: 71\n",
      "Predictions for pf_rate saved to ./predictions_pf_rate.csv\n",
      "Starting predictions for mixed_rate\n",
      "Predictions count: 71\n",
      "Predictions for mixed_rate saved to ./predictions_mixed_rate.csv\n",
      "Predictions count: 71\n",
      "Predictions for mixed_rate saved to ./predictions_mixed_rate.csv\n"
     ]
    }
   ],
   "source": [
    "# For each target, load the model and predict\n",
    "targets = [\"pv_rate\", \"pf_rate\", \"mixed_rate\"]\n",
    "for tgt in targets:\n",
    "    print(f\"Starting predictions for {tgt}\")\n",
    "    out_dir = f\"./final_output/malaria_e2e/Target_{tgt.split('_')[0].upper()}_over_Pop\"\n",
    "    # Load VAE\n",
    "    vae = CondVAE(inp=len(INDICATORS),hid=HIDDEN_MULT*len(INDICATORS),lat=LATENT,n_upazila=full_df[\"upazila_code\"].max()+1,n_year=full_df[\"year_code\"].max()+1)\n",
    "    vae.load_state_dict(torch.load(os.path.join(out_dir,\"conditional_vae.pt\"), map_location=DEVICE))\n",
    "    vae.to(DEVICE)\n",
    "\n",
    "    # For scalers, process hist_df\n",
    "    lat_hist = vae_latent_df(vae, hist_df)\n",
    "    base_hist = lat_hist.merge(hist_df[[\"UpazilaID\",\"year\",\"month\",\"ym\"] + INDICATORS + [tgt]], on=[\"UpazilaID\",\"year\",\"month\",\"ym\"], how=\"left\")\n",
    "    base_hist, lag_cols = add_lags(base_hist, INDICATORS, group_col=\"UpazilaID\", lags=(1,3,6), rolls=(3,6))\n",
    "    pc_cols = [c for c in base_hist.columns if c.startswith(\"pc_mean_\") or c.startswith(\"pc_std_\")]\n",
    "    feats = pc_cols + [\"month_sin\",\"month_cos\"] + lag_cols\n",
    "    # Load training data to fit scalers\n",
    "    split_tr, le_tr, fsc_tr, ysc_tr = build_mats(base_hist, feats, tgt, \"UpazilaID\")\n",
    "\n",
    "    # Get latents for full\n",
    "    lat = vae_latent_df(vae, full_df)\n",
    "    # Merge\n",
    "    merge_cols = [\"UpazilaID\",\"year\",\"month\",\"ym\"] + INDICATORS\n",
    "    if tgt in full_df.columns:\n",
    "        merge_cols += [tgt]\n",
    "    base = lat.merge(full_df[merge_cols], on=[\"UpazilaID\",\"year\",\"month\",\"ym\"], how=\"left\")\n",
    "    # Add lags\n",
    "    base, lag_cols = add_lags(base, INDICATORS, group_col=\"UpazilaID\", lags=(1,3,6), rolls=(3,6))\n",
    "    # Feats (same as above)\n",
    "    pc_cols = [c for c in base.columns if c.startswith(\"pc_mean_\") or c.startswith(\"pc_std_\")]\n",
    "    feats = pc_cols + [\"month_sin\",\"month_cos\"] + lag_cols\n",
    "\n",
    "    # Load Generator\n",
    "    nc = int(split_tr.c.max()) + 1\n",
    "    cond_dim = len(feats) + 1  # + prev_y\n",
    "    G = Generator(cond_dim=cond_dim,noise_dim=NOISE,nc=nc)\n",
    "    G.load_state_dict(torch.load(os.path.join(out_dir,\"generator.pt\"), map_location=DEVICE))\n",
    "    G.to(DEVICE)\n",
    "    G.eval()\n",
    "    # Now, for each upazila, forecast the next month\n",
    "    predictions = []\n",
    "    for uid in hist_df[\"UpazilaID\"].unique():\n",
    "        g = base[base[\"UpazilaID\"] == uid].sort_values([\"year\",\"month\"]).copy()\n",
    "        if len(g) < SEQ:\n",
    "            continue\n",
    "        # Take last SEQ\n",
    "        last_seq = g.iloc[-SEQ:].copy()\n",
    "        # Prepare X\n",
    "        X = last_seq[feats].values.astype(np.float32)\n",
    "        Xs = fsc_tr.transform(X)\n",
    "        # prev_y, start with 0\n",
    "        prev_y = np.zeros((SEQ,1), np.float32)\n",
    "        X_full = np.concatenate([Xs, prev_y], axis=1)\n",
    "        # Predict next\n",
    "        X_t = torch.tensor(X_full[np.newaxis, :, :], device=DEVICE)\n",
    "        C_t = torch.tensor([le_tr.transform([str(uid)])[0]], device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            mu, _, _ = G(X_t, torch.zeros(1, SEQ, NOISE, device=DEVICE), C_t)\n",
    "        pred_scaled = mu.cpu().numpy()[0, -1, 0]\n",
    "        pred_rate = np.expm1(ysc_tr[int(C_t.item())].inverse_transform([[pred_scaled]])[0,0])\n",
    "        predictions.append({\"UpazilaID\": uid, \"predicted_rate\": pred_rate, \"target\": tgt})\n",
    "    print(f\"Predictions count: {len(predictions)}\")\n",
    "    pred_df_out = pd.DataFrame(predictions)\n",
    "    pred_df_out.to_csv(f\"./predictions_{tgt}.csv\", index=False)\n",
    "    print(f\"Predictions for {tgt} saved to ./predictions_{tgt}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca42df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load malaria data ...\n",
      "\n",
      "========== Target: pv_rate ==========\n",
      "1) Train Conditional VAE on indicators ...\n",
      "2) Extract latent features ...\n",
      "3) Build lag features (1,3,6) + rolling stats ...\n",
      "4) Split per Upazila: train / val (-12:-6) / test (-6:) ...\n",
      "5) Fit scalers/encoders on TRAIN ...\n",
      "Train sequences: (9443, 12, 62)\n",
      "6) Train forecasting GAN ...\n",
      "7) Evaluate on HELD-OUT TEST ...\n",
      "\n",
      "=== HELD-OUT TEST ===\n",
      "SMAPE: 47.0294\n",
      "MSE: 0.0000\n",
      "RMSE: 0.0000\n",
      "R2: 0.9863\n",
      "Coverage90: 0.6609\n",
      "[pv_rate] Artifacts saved to ./output/malaria_e2e/Target_PV_over_Pop\n",
      "\n",
      "========== Target: pf_rate ==========\n",
      "1) Train Conditional VAE on indicators ...\n",
      "2) Extract latent features ...\n",
      "3) Build lag features (1,3,6) + rolling stats ...\n",
      "4) Split per Upazila: train / val (-12:-6) / test (-6:) ...\n",
      "5) Fit scalers/encoders on TRAIN ...\n",
      "Train sequences: (9443, 12, 62)\n",
      "6) Train forecasting GAN ...\n",
      "7) Evaluate on HELD-OUT TEST ...\n",
      "\n",
      "=== HELD-OUT TEST ===\n",
      "SMAPE: 59.2943\n",
      "MSE: 0.0000\n",
      "RMSE: 0.0001\n",
      "R2: 0.9975\n",
      "Coverage90: 0.7667\n",
      "[pf_rate] Artifacts saved to ./output/malaria_e2e/Target_PF_over_Pop\n",
      "\n",
      "========== Target: mixed_rate ==========\n",
      "1) Train Conditional VAE on indicators ...\n",
      "2) Extract latent features ...\n",
      "3) Build lag features (1,3,6) + rolling stats ...\n",
      "4) Split per Upazila: train / val (-12:-6) / test (-6:) ...\n",
      "5) Fit scalers/encoders on TRAIN ...\n",
      "Train sequences: (9443, 12, 62)\n",
      "6) Train forecasting GAN ...\n"
     ]
    }
   ],
   "source": [
    "# ===== Malaria Upazila-wise forecasting (PV/Pop, PF/Pop, MIXED/Pop) =====\n",
    "import os, math, json, copy, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from math import erf\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "MALARIA_CSV = \"./TrainingFile2012_2024.csv\"   # <<< set your path\n",
    "OUT_ROOT = \"./output/malaria_e2e\"; os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "SEED=42; DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEQ=12; BATCH=64; EPOCHS_VAE=400; EPOCHS_GAN=400\n",
    "LATENT=12; HIDDEN_MULT=4; DROP=0.25; BETA=2.0; CONTR=1e-3; PATIENCE=25\n",
    "NOISE=16; LSTM_UNITS=128; HEADS=8\n",
    "LR_VAE=1e-3; LR_G=8e-4; LR_D=3e-4; WD=1e-4; CLIP=1.0; TTUR=(1.0,1.0)\n",
    "TF_START=1.0; TF_END=0.3\n",
    "ALPHA=0.5; K_SYNC=5; TOPK=10\n",
    "Q=(0.1,0.5,0.9)\n",
    "ABL={\"adv\":True,\"hetero\":True,\"quant\":True}\n",
    "NSIG=(0.1,1.2); K_MC=50\n",
    "\n",
    "# indicators provided in your sample\n",
    "INDICATORS = [\n",
    "    \"Average_temperature\",\"Total_rainfall\",\"Relative_humidity\",\"Average_NDVI\",\"Average_NDWI\"\n",
    "]\n",
    "\n",
    "VAL_H_MONTHS  = 6    # per Upazila for early stop\n",
    "TEST_H_MONTHS = 6    # per Upazila for final test\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark=False; torch.backends.cudnn.deterministic=True\n",
    "\n",
    "# ----------------- Utils -----------------\n",
    "def smape(y, p): y=y.flatten(); p=p.flatten(); return 100*np.mean(2*np.abs(p-y)/(np.abs(y)+np.abs(p)+1e-8))\n",
    "def lerp(a,b,t): return {k:a[k]+(b[k]-a[k])*t for k in a}\n",
    "def psd(M): e,V=np.linalg.eigh(M); e[e<1e-6]=1e-6; return (V@np.diag(e)@V.T).astype(np.float32)\n",
    "\n",
    "# ----------------- Data (Malaria) -----------------\n",
    "def load_malaria(path):\n",
    "    \"\"\"\n",
    "    Expected columns (at minimum):\n",
    "    UpazilaID, month, year, Population, PV, PF, MIXED, and the 5 indicators.\n",
    "    Optional: DIS_NAME/DIS_CODE/UPA_NAME etc. are ignored.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    # normalize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # safety: standard names\n",
    "    colmap = {\n",
    "        \"Month\":\"month\",\"MonthNo\":\"month\",\"MONTH\":\"month\",\n",
    "        \"Year\":\"year\",\"YEAR\":\"year\",\n",
    "        \"UpazilaId\":\"UpazilaID\",\"UPAZILAID\":\"UpazilaID\",\"UpazilaID\":\"UpazilaID\"\n",
    "    }\n",
    "    for k,v in colmap.items():\n",
    "        if k in df.columns and v not in df.columns:\n",
    "            df[v]=df[k]\n",
    "    # types\n",
    "    df[\"UpazilaID\"] = pd.to_numeric(df[\"UpazilaID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(int)\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"],  errors=\"coerce\").astype(int)\n",
    "\n",
    "    # build targets (rates)\n",
    "    for cases_col, tgt in [(\"PV\",\"pv_rate\"),(\"PF\",\"pf_rate\"),(\"MIXED\",\"mixed_rate\")]:\n",
    "        if cases_col in df.columns:\n",
    "            cc = pd.to_numeric(df[cases_col], errors=\"coerce\")\n",
    "            pp = pd.to_numeric(df[\"Population\"], errors=\"coerce\")\n",
    "            m  = (pp>0) & cc.notna()\n",
    "            df[tgt] = np.nan\n",
    "            df.loc[m, tgt] = (cc[m] / pp[m]).astype(float)\n",
    "        else:\n",
    "            df[tgt] = np.nan\n",
    "\n",
    "    # order & date key\n",
    "    df = df.sort_values([\"UpazilaID\",\"year\",\"month\"]).reset_index(drop=True)\n",
    "    df[\"ym\"] = df[\"year\"]*12 + df[\"month\"]\n",
    "\n",
    "    # forward-fill per Upazila for indicators + population + targets\n",
    "    cols_to_ffill = set(INDICATORS + [\"Population\",\"pv_rate\",\"pf_rate\",\"mixed_rate\"])\n",
    "    present = [c for c in cols_to_ffill if c in df.columns]\n",
    "    df[present] = df.groupby(\"UpazilaID\")[present].apply(lambda g: g.ffill()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # replace inf -> NaN -> ffill (still per Upazila)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df[present] = df.groupby(\"UpazilaID\")[present].apply(lambda g: g.ffill()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # seasonality\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12.).astype(np.float32)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12.).astype(np.float32)\n",
    "\n",
    "    # codes for conditioning VAE\n",
    "    df[\"upazila_code\"] = pd.Categorical(df[\"UpazilaID\"]).codes\n",
    "    df[\"year_code\"]    = pd.Categorical(df[\"year\"]).codes\n",
    "    return df\n",
    "\n",
    "# ----------------- Conditional VAE on indicators -----------------\n",
    "class CondVAE(nn.Module):\n",
    "    def __init__(s,inp,hid,lat,n_upazila,n_year,emb=16):\n",
    "        super().__init__()\n",
    "        s.ue=nn.Embedding(n_upazila,emb); s.ye=nn.Embedding(n_year,emb)\n",
    "        s.p=nn.Linear(inp,hid//2); s.f=nn.Linear(hid//2+2*emb,hid)\n",
    "        s.mu=nn.Linear(hid,lat); s.lv=nn.Linear(hid,lat)\n",
    "        s.fd=nn.Linear(lat+2*emb,hid); s.out=nn.Linear(hid,inp)\n",
    "        s.l1=nn.LayerNorm(hid); s.l2=nn.LayerNorm(hid); s.drop=nn.Dropout(DROP)\n",
    "    def encode(s,x,u,y):\n",
    "        h=torch.relu(s.p(x)); h=torch.cat([h,s.ue(u),s.ye(y)],1)\n",
    "        h=s.drop(torch.relu(s.l1(s.f(h)))); return s.mu(h), s.lv(h)\n",
    "    def reparam(s,m,l): std=(0.5*l).exp(); return m+torch.randn_like(std)*std\n",
    "    def decode(s,z,u,y):\n",
    "        h=torch.cat([z,s.ue(u),s.ye(y)],1); h=s.drop(torch.relu(s.l2(s.fd(h))))\n",
    "        return s.out(h)\n",
    "    def forward(s,x,u,y): mu,lv=s.encode(x,u,y); z=s.reparam(mu,lv); return s.decode(z,u,y),mu,lv\n",
    "\n",
    "def vae_loss(xh,x,mu,lv,model,x_in,u,y):\n",
    "    recon=F.mse_loss(xh,x)\n",
    "    kl=-0.5*torch.mean(1+lv-mu.pow(2)-lv.exp())\n",
    "    x_in.requires_grad_(True); mu_c,_=model.encode(x_in,u,y)\n",
    "    g=torch.autograd.grad(mu_c.sum(),x_in,create_graph=True)[0]\n",
    "    return recon+BETA*kl+CONTR*(g.pow(2).sum(1).mean())\n",
    "\n",
    "def train_vae_on_indicators(df, out_dir):\n",
    "    # build matrix of indicators\n",
    "    X = df[INDICATORS].astype(np.float32).values\n",
    "    u = df[\"upazila_code\"].astype(np.int64).values\n",
    "    y = df[\"year_code\"].astype(np.int64).values\n",
    "\n",
    "    # train/val split for VAE: earlier years train, later years val\n",
    "    yq=np.quantile(df[\"year\"],0.8)\n",
    "    mtr = (df[\"year\"]<=yq)\n",
    "\n",
    "    Xt=torch.tensor(X[mtr],dtype=torch.float32,device=DEVICE)\n",
    "    Xv=torch.tensor(X[~mtr],dtype=torch.float32,device=DEVICE)\n",
    "    ut=torch.tensor(u[mtr],dtype=torch.long,device=DEVICE)\n",
    "    yt=torch.tensor(y[mtr],dtype=torch.long,device=DEVICE)\n",
    "    uv=torch.tensor(u[~mtr],dtype=torch.long,device=DEVICE)\n",
    "    yv=torch.tensor(y[~mtr],dtype=torch.long,device=DEVICE)\n",
    "\n",
    "    inp=Xt.shape[1]; hid=HIDDEN_MULT*inp\n",
    "    n_upazila=int(df[\"upazila_code\"].max())+1\n",
    "    n_year   =int(df[\"year_code\"].max())+1\n",
    "    vae=CondVAE(inp,hid,LATENT,n_upazila,n_year).to(DEVICE)\n",
    "\n",
    "    opt=torch.optim.AdamW(vae.parameters(),lr=LR_VAE,weight_decay=WD)\n",
    "    sch=torch.optim.lr_scheduler.ReduceLROnPlateau(opt,\"min\",0.5,5)\n",
    "    best=float(\"inf\"); best_sd=None; wait=0\n",
    "    for e in range(EPOCHS_VAE):\n",
    "        vae.train(); opt.zero_grad(set_to_none=True)\n",
    "        xh,mu,lv=vae(Xt,ut,yt); loss=vae_loss(xh,Xt,mu,lv,vae,Xt.clone(),ut,yt); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(),CLIP); opt.step()\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            xhv,mv,lvv=vae(Xv,uv,yv)\n",
    "            vloss=F.mse_loss(xhv,Xv)+BETA*(-0.5*torch.mean(1+lvv-mv.pow(2)-lvv.exp()))\n",
    "        sch.step(vloss)\n",
    "        if vloss.item()<best-1e-6:\n",
    "            best=vloss.item(); best_sd=copy.deepcopy(vae.state_dict()); wait=0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if e>=30 and wait>=PATIENCE: break\n",
    "    if best_sd: vae.load_state_dict(best_sd)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    torch.save(vae.state_dict(), os.path.join(out_dir,\"conditional_vae.pt\"))\n",
    "    return vae\n",
    "\n",
    "def vae_latent_df(vae, df):\n",
    "    vae.eval()\n",
    "    X = torch.tensor(df[INDICATORS].astype(np.float32).values, device=DEVICE)\n",
    "    U = torch.tensor(df[\"upazila_code\"].astype(np.int64).values, device=DEVICE)\n",
    "    Y = torch.tensor(df[\"year_code\"].astype(np.int64).values, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _,mu,_ = vae(X,U,Y)\n",
    "    Z = mu.detach().cpu().numpy().astype(np.float32)\n",
    "    out = df[[\"UpazilaID\",\"year\",\"month\",\"ym\"]].copy()\n",
    "    # attach direct means/std (std not well-defined from single pass → zeros)\n",
    "    for i in range(LATENT):\n",
    "        out[f\"pc_mean_{i+1}\"] = Z[:,i]\n",
    "        out[f\"pc_std_{i+1}\"]  = 0.0\n",
    "    out[\"month_sin\"] = df[\"month_sin\"].values.astype(np.float32)\n",
    "    out[\"month_cos\"] = df[\"month_cos\"].values.astype(np.float32)\n",
    "    return out\n",
    "\n",
    "# --------------- Lag features ----------------\n",
    "def add_lags(df, base_cols, group_col=\"UpazilaID\", lags=(1,3,6), rolls=(3,6)):\n",
    "    df = df.sort_values([group_col,\"year\",\"month\"]).copy()\n",
    "    new_cols=[]\n",
    "    for c in base_cols:\n",
    "        for L in lags:\n",
    "            col=f\"{c}_lag{L}\"\n",
    "            df[col]=df.groupby(group_col)[c].shift(L); new_cols.append(col)\n",
    "        for R in rolls:\n",
    "            m=f\"{c}_rmean{R}\"; s=f\"{c}_rstd{R}\"\n",
    "            g=df.groupby(group_col)[c]\n",
    "            df[m]=g.rolling(R,min_periods=1).mean().reset_index(level=0,drop=True)\n",
    "            df[s]=g.rolling(R,min_periods=1).std().reset_index(level=0,drop=True).fillna(0.0)\n",
    "            new_cols += [m,s]\n",
    "    df[new_cols] = df[new_cols].fillna(0.0)\n",
    "    return df, new_cols\n",
    "\n",
    "# ----------------- Seq utilities -----------------\n",
    "class Split:\n",
    "    def __init__(s,X,y,c,df): s.X=X; s.y=y; s.c=c; s.df=df\n",
    "\n",
    "def build_mats(df, feat_cols, target_col, entity_col=\"UpazilaID\"):\n",
    "    le = LabelEncoder().fit(df[entity_col].astype(str).values)\n",
    "    d  = df.copy()\n",
    "    d[\"entity_id\"] = le.transform(d[entity_col].astype(str))\n",
    "\n",
    "    X = d[feat_cols].values.astype(np.float32)\n",
    "    feat_scaler = StandardScaler().fit(X)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    # log1p target (non-negative rates)\n",
    "    y  = np.log1p(np.clip(d[target_col].values.reshape(-1,1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y, np.float32)\n",
    "\n",
    "    y_scalers = {}\n",
    "    for cid, g in d.groupby(\"entity_id\"):\n",
    "        idx = g.index.values\n",
    "        sc  = StandardScaler().fit(y[idx])\n",
    "        y_scalers[cid] = sc\n",
    "        Ys[idx] = sc.transform(y[idx])\n",
    "\n",
    "    return Split(Xs, Ys, d[\"entity_id\"].values.astype(np.int64), d), le, feat_scaler, y_scalers\n",
    "\n",
    "def apply_mats(df, feat_cols, target_col, le, feat_scaler, y_scalers, entity_col=\"UpazilaID\"):\n",
    "    d = df.copy()\n",
    "    d[\"entity_id\"] = le.transform(d[entity_col].astype(str))\n",
    "    X = d[feat_cols].values.astype(np.float32)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    y  = np.log1p(np.clip(d[target_col].values.reshape(-1,1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y, np.float32)\n",
    "    for cid, g in d.groupby(\"entity_id\"):\n",
    "        idx = g.index.values\n",
    "        sc = y_scalers.get(int(cid), StandardScaler().fit(y[idx]))\n",
    "        Ys[idx] = sc.transform(y[idx])\n",
    "\n",
    "    return Split(Xs, Ys, d[\"entity_id\"].values.astype(np.int64), d)\n",
    "\n",
    "def to_seq(split,L=SEQ,prev_y=True):\n",
    "    X,y,c,df=split.X,split.y,split.c,split.df\n",
    "    ym=(df[\"year\"].astype(int)*12+df[\"month\"].astype(int)).values\n",
    "    SX,SY,SC=[],[],[]\n",
    "    for cid in np.unique(c):\n",
    "        idx=np.where(c==cid)[0]; idx=idx[np.argsort(ym[idx])]; o=ym[idx]\n",
    "        for i in range(len(idx)-L+1):\n",
    "            sl=idx[i:i+L]\n",
    "            if np.all(np.diff(o[i:i+L])==1):\n",
    "                Xi=X[sl]; Yi=y[sl]\n",
    "                if prev_y:\n",
    "                    prev=np.vstack([np.zeros((1,1),np.float32),Yi[:-1]])\n",
    "                    Xi=np.concatenate([Xi,prev],1)\n",
    "                SX.append(Xi); SY.append(Yi); SC.append(cid)\n",
    "    return np.asarray(SX,np.float32), np.asarray(SY,np.float32), np.asarray(SC,np.int64)\n",
    "\n",
    "# ----------------- Models (same as your dengue GAN) -----------------\n",
    "class CausalTCN(nn.Module):\n",
    "    def __init__(s,in_ch,hid=64,levels=3,k=3):\n",
    "        super().__init__(); s.k=k; s.blocks=nn.ModuleList(); ch=in_ch\n",
    "        for l in range(levels):\n",
    "            dil=2**l; pad=(k-1)*dil\n",
    "            s.blocks.append(nn.Sequential(nn.Conv1d(ch,hid,kernel_size=k,dilation=dil,padding=pad),nn.GELU())); ch=hid\n",
    "    def forward(s,x):\n",
    "        y=x.transpose(1,2); L=y.size(-1)\n",
    "        for _,b in enumerate(s.blocks):\n",
    "            y=b(y); y=y[...,:L]\n",
    "        return y.transpose(1,2)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(s,cond_dim,noise_dim,nc,emb=8,lstm=LSTM_UNITS,heads=HEADS,drop=DROP,qu=Q):\n",
    "        super().__init__(); s.q=qu; s.ce=nn.Embedding(nc,emb)\n",
    "        s.pn=nn.Linear(noise_dim,cond_dim); s.tcn=CausalTCN(cond_dim*2+emb,hid=lstm)\n",
    "        s.lstm=nn.LSTM(lstm,lstm,1,batch_first=True)\n",
    "        s.mha=nn.MultiheadAttention(lstm,heads,batch_first=True,dropout=drop)\n",
    "        s.ln=nn.LayerNorm(lstm); s.drop=nn.Dropout(drop)\n",
    "        s.mu=nn.Linear(lstm,1); s.ls=nn.Linear(lstm,1)\n",
    "        s.qh=nn.ModuleList([nn.Linear(lstm,1) for _ in qu]); s.last=None\n",
    "    def forward(s,cond,noise,cid):\n",
    "        B,L,D=cond.shape; emb=s.ce(cid).unsqueeze(1).repeat(1,L,1)\n",
    "        z=s.pn(noise); h=s.tcn(torch.cat([cond,z,emb],-1)); h,_=s.lstm(h)\n",
    "        L = h.size(1)\n",
    "        mask = torch.triu(torch.ones(L, L, device=h.device, dtype=torch.bool), diagonal=1)\n",
    "        att, w = s.mha(h, h, h, attn_mask=mask, need_weights=True)\n",
    "        s.last = w.detach()\n",
    "        h=s.drop(s.ln(att)); mu=s.mu(h); ls=torch.clamp(s.ls(h),-5.,3.); qs=[q(h) for q in s.qh]\n",
    "        return mu,ls,qs\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(s,cond_dim,nc,emb=8,lstm=LSTM_UNITS,drop=DROP):\n",
    "        super().__init__(); s.ce=nn.Embedding(nc,emb)\n",
    "        s.lstm=nn.LSTM(cond_dim+1+emb,lstm,1,batch_first=True); s.fc=nn.Linear(lstm,64); s.drop=nn.Dropout(drop); s.out=nn.Linear(64,1)\n",
    "    def forward(s,cond,ts,c):\n",
    "        B,L,D=cond.shape\n",
    "        e=s.ce(c).unsqueeze(1).repeat(1,L,1)\n",
    "        h,_=s.lstm(torch.cat([cond,ts,e],-1)); f=F.gelu(s.fc(h[:,-1,:]));\n",
    "        return s.out(s.drop(f)).squeeze(1), f\n",
    "\n",
    "def pinball(p,t,q): e=t-p; return torch.mean(torch.maximum(q*e,(q-1)*e))\n",
    "def tv_l1(x): d=x[:,1:]-x[:,:-1]; return d.abs().mean(), x.abs().mean()\n",
    "\n",
    "def gp(critic,yr,yf,cond,cc,lam=10.):\n",
    "    B=yr.size(0); eps=torch.rand(B,1,1,device=DEVICE); xi=(eps*yr+(1-eps)*yf).requires_grad_(True)\n",
    "    with torch.backends.cudnn.flags(enabled=False): s,_=critic(cond,xi,cc)\n",
    "    g=torch.autograd.grad(s,xi,grad_outputs=torch.ones_like(s),create_graph=True,retain_graph=True,only_inputs=True)[0]\n",
    "    return ((g.view(B,-1).norm(2,1)-1)**2).mean()*lam\n",
    "\n",
    "class Lookahead:\n",
    "    def __init__(s,opt,alpha=ALPHA,k=K_SYNC): s.opt=opt; s.alpha=alpha; s.k=k; s.stepn=0; s.slow={id(p):p.clone().detach() for g in opt.param_groups for p in g[\"params\"] if p.requires_grad}\n",
    "    def zero_grad(s,set_to_none=True): s.opt.zero_grad(set_to_none=set_to_none)\n",
    "    def step(s):\n",
    "        s.opt.step(); s.stepn+=1\n",
    "        if s.stepn%s.k==0:\n",
    "            for g in s.opt.param_groups:\n",
    "                for p in g[\"params\"]:\n",
    "                    if not p.requires_grad: continue\n",
    "                    sp=s.slow[id(p)]; sp.data.add_(s.alpha,p.data-sp.data); p.data.copy_(sp.data)\n",
    "\n",
    "class SAM:\n",
    "    def __init__(s,opt,rho=0.05,adaptive=True): s.b=opt; s.rho=rho; s.adapt=adaptive; s.eps=None\n",
    "    def _gn(s,ps):\n",
    "        ns=[((p.abs()*p.grad) if s.adapt else p.grad).norm(p=2) for p in ps if p.grad is not None]\n",
    "        return torch.norm(torch.stack(ns),p=2) if ns else torch.tensor(0.,device=DEVICE)\n",
    "    def first(s,ps):\n",
    "        scale=s.rho/(s._gn(ps)+1e-12); s.eps=[]\n",
    "        with torch.no_grad():\n",
    "            for p in ps:\n",
    "                if p.grad is None: s.eps.append(None); continue\n",
    "                e=((p.abs()*p.grad) if s.adapt else p.grad)*scale; p.add_(e); s.eps.append(e)\n",
    "        s.b.zero_grad(set_to_none=True)\n",
    "    def second(s,ps):\n",
    "        with torch.no_grad():\n",
    "            for p,e in zip(ps,s.eps or []):\n",
    "                if e is not None: p.sub_(e)\n",
    "        torch.nn.utils.clip_grad_norm_(ps,CLIP); s.b.step(); s.b.zero_grad(set_to_none=True); s.eps=None\n",
    "\n",
    "class SeqDS(Dataset):\n",
    "    def __init__(s,X,Y,C): s.X=X; s.Y=Y; s.C=C\n",
    "    def __len__(s): return s.X.shape[0]\n",
    "    def __getitem__(s,i): return s.X[i],s.Y[i],s.C[i]\n",
    "\n",
    "def _g_loss(G,D,X,Y,C,z,w,pc_idx):\n",
    "    mu,ls,qs=G(X,z,C); sig=(ls.exp()).clamp(1e-3,50.0)\n",
    "    nll=0.5*(((Y-mu)/sig)**2+2*ls+math.log(2*math.pi)).mean() if ABL[\"hetero\"] else F.l1_loss(mu,Y)\n",
    "    ql=torch.tensor(0.,device=DEVICE)\n",
    "    if ABL[\"quant\"]:\n",
    "        for i,q in enumerate(Q): ql+=pinball(qs[i],Y,q)\n",
    "        ql/=len(Q)\n",
    "    tv,l1=tv_l1(X[...,pc_idx])\n",
    "    fm=torch.tensor(0.,device=DEVICE); adv_term=torch.tensor(0.,device=DEVICE)\n",
    "    if ABL[\"adv\"]:\n",
    "        with torch.no_grad(): _, fr=D(X,Y,C)\n",
    "        s_fake, ff=D(X,mu,C); fm=F.l1_loss(ff,fr); adv_term=-s_fake.mean()\n",
    "    return w[\"nll\"]*nll+w[\"q\"]*ql+w[\"tv\"]*tv+w[\"l1\"]*l1+(w.get(\"fm\",0)*fm if ABL[\"adv\"] else 0.)+(w.get(\"adv\",0)*adv_term if ABL[\"adv\"] else 0.)\n",
    "\n",
    "def validate(G,Xv,Yv,Cv):\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        X=torch.tensor(Xv,dtype=torch.float32,device=DEVICE); C=torch.tensor(Cv,dtype=torch.long,device=DEVICE)\n",
    "        B,L,_=X.shape; mu,ls,qs=G(X,torch.zeros(B,L,NOISE,device=DEVICE),C)\n",
    "        mp=mu[...,0].cpu().numpy(); y=Yv[...,0]\n",
    "        q10=qs[0][...,0].cpu().numpy(); q90=qs[2][...,0].cpu().numpy()\n",
    "        cov=float(np.mean((y>=q10)&(y<=q90))); sm=smape(y.reshape(-1),mp.reshape(-1))\n",
    "    return sm,cov\n",
    "\n",
    "def train_gan(Xtr,Ytr,Ctr,Xva,Yva,Cva,cond_dim,nc,pc_idx,out_dir):\n",
    "    G=Generator(cond_dim=cond_dim,noise_dim=NOISE,nc=nc).to(DEVICE)\n",
    "    D=Critic(cond_dim=cond_dim,nc=nc).to(DEVICE)\n",
    "    optG=Lookahead(torch.optim.AdamW(G.parameters(),lr=LR_G*TTUR[0],betas=(0.9,0.999),weight_decay=WD))\n",
    "    optD=Lookahead(torch.optim.AdamW(D.parameters(),lr=LR_D*TTUR[1],betas=(0.9,0.999),weight_decay=WD))\n",
    "    sam=SAM(optG,0.05,True)\n",
    "    dl=DataLoader(SeqDS(Xtr,Ytr,Ctr),batch_size=BATCH,shuffle=True,drop_last=True)\n",
    "    best=float(\"inf\"); best_sd=None; wait=0\n",
    "    for e in range(EPOCHS_GAN):\n",
    "        t=e/(EPOCHS_GAN-1); ns=NSIG[0]+(NSIG[1]-NSIG[0])*t\n",
    "        W_START = {\"nll\":1.0,\"q\":0.5,\"tv\":0.05,\"l1\":0.02,\"fm\":0.2,\"adv\":0.5}\n",
    "        W_END   = {\"nll\":1.0,\"q\":1.0,\"tv\":0.10,\"l1\":0.05,\"fm\":0.1,\"adv\":0.4}\n",
    "        w=lerp(W_START,W_END,t); tf=TF_START+(TF_END-TF_START)*t\n",
    "        G.train(); D.train()\n",
    "        for Xb,Yb,Cb in dl:\n",
    "            Xb=torch.tensor(Xb,dtype=torch.float32,device=DEVICE); Yb=torch.tensor(Yb,dtype=torch.float32,device=DEVICE); Cb=torch.tensor(Cb,dtype=torch.long,device=DEVICE)\n",
    "            B,L,_=Xb.shape\n",
    "            with torch.no_grad():\n",
    "                mu0,_,_=G(Xb,torch.zeros(B,L,NOISE,device=DEVICE),Cb)\n",
    "                prev=torch.cat([torch.zeros(B,1,device=DEVICE),mu0[:,:-1,0]],1)\n",
    "            Xb[:,:,-1]=tf*Xb[:,:,-1]+(1-tf)*prev\n",
    "            # Critic\n",
    "            if ABL[\"adv\"]:\n",
    "                optD.zero_grad(set_to_none=True)\n",
    "                z=torch.randn(B,L,NOISE,device=DEVICE)*ns\n",
    "                mu,_,_=G(Xb,z,Cb); Yf=mu.detach(); r,_=D(Xb,Yb,Cb); f,_=D(Xb,Yf,Cb)\n",
    "                dloss=-(r.mean()-f.mean())+gp(D,Yb,Yf,Xb,Cb,10.0); dloss.backward(); torch.nn.utils.clip_grad_norm_(D.parameters(),CLIP); optD.step()\n",
    "            # Generator + SAM\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            L1=_g_loss(G,D,Xb,Yb,Cb,torch.randn(B,L,NOISE,device=DEVICE)*ns,w,pc_idx); L1.backward(); sam.first(list(G.parameters()))\n",
    "            L2=_g_loss(G,D,Xb,Yb,Cb,torch.randn(B,L,NOISE,device=DEVICE)*ns,w,pc_idx); L2.backward(); sam.second(list(G.parameters()))\n",
    "        sm,cov=validate(G,Xva,Yva,Cva); comp=sm+10*abs(cov-0.9)\n",
    "        if comp<best-1e-6: best=comp; best_sd=(copy.deepcopy(G.state_dict()),copy.deepcopy(D.state_dict())); wait=0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait>=PATIENCE: break\n",
    "    if best_sd: G.load_state_dict(best_sd[0]); D.load_state_dict(best_sd[1])\n",
    "    torch.save(G.state_dict(),os.path.join(out_dir,\"generator.pt\")); torch.save(D.state_dict(),os.path.join(out_dir,\"critic.pt\"))\n",
    "    return G,D\n",
    "\n",
    "# --------------- Prob bands ---------------\n",
    "def copula_bands(G,X,C,K=K_MC):\n",
    "    G.eval(); X=torch.tensor(X,dtype=torch.float32,device=DEVICE); C=torch.tensor(C,dtype=torch.long,device=DEVICE)\n",
    "    with torch.no_grad(): mu,ls,_=G(X,torch.zeros(X.shape[0],X.shape[1],NOISE,device=DEVICE),C)\n",
    "    mu=mu.cpu().numpy()[...,0]; sig=np.clip(np.exp(ls.cpu().numpy()[...,0]),1e-3,50.0)\n",
    "    N,L=X.shape[0],X.shape[1]; S=[]\n",
    "    for i in range(N):\n",
    "        rho=0.5; R=np.fromfunction(lambda a,b: rho**np.abs(a-b),(L,L)); Sg=psd((sig[i][:,None]*sig[i][None,:])*R)\n",
    "        z=np.random.multivariate_normal(np.zeros(L),Sg,size=K).astype(np.float32); S.append(mu[i][None,:]+z)\n",
    "    S=np.stack(S,0); return np.percentile(S,10,1),np.percentile(S,50,1),np.percentile(S,90,1)\n",
    "\n",
    "# --------------- Eval ---------------\n",
    "def evaluate(G, Xseq, Yseq, Cseq, ysc, le, label=\"Evaluation\"):\n",
    "    G.eval()\n",
    "    X = torch.tensor(Xseq, dtype=torch.float32, device=DEVICE)\n",
    "    C = torch.tensor(Cseq, dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mu, ls, qs = G(X, torch.zeros(X.shape[0], X.shape[1], NOISE, device=DEVICE), C)\n",
    "        mp = mu.cpu().numpy()[..., 0]\n",
    "    q10, _, q90 = copula_bands(G, Xseq, Cseq)\n",
    "    yF=[]; mF=[]; lF=[]; hF=[]\n",
    "    for i, cid in enumerate(Cseq):\n",
    "        sc = ysc[int(cid)]\n",
    "        y  = np.expm1(sc.inverse_transform(Yseq[i].reshape(-1,1)).reshape(-1))\n",
    "        m  = np.clip(np.expm1(sc.inverse_transform(mp[i].reshape(-1,1)).reshape(-1)), 0, None)\n",
    "        lo = np.clip(np.expm1(sc.inverse_transform(q10[i].reshape(-1,1)).reshape(-1)), 0, None)\n",
    "        hi = np.clip(np.expm1(sc.inverse_transform(q90[i].reshape(-1,1)).reshape(-1)), 0, None)\n",
    "        yF.append(y); mF.append(m); lF.append(lo); hF.append(hi)\n",
    "    yF = np.concatenate(yF); mF = np.concatenate(mF); lF = np.concatenate(lF); hF = np.concatenate(hF)\n",
    "    overall = {\n",
    "        \"SMAPE\": smape(yF, mF),\n",
    "        \"MSE\": mean_squared_error(yF, mF),\n",
    "        \"RMSE\": math.sqrt(mean_squared_error(yF, mF)),\n",
    "        \"R2\": r2_score(yF, mF),\n",
    "        \"Coverage90\": float(np.mean((yF >= lF) & (yF <= hF))),\n",
    "    }\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    for k,v in overall.items(): print(f\"{k}: {v:.4f}\")\n",
    "    return overall, (mF, lF, hF, yF)\n",
    "\n",
    "# --------------- Main per-target ---------------\n",
    "def run_for_target(df_raw, target_col, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"\\n========== Target: {target_col} ==========\")\n",
    "\n",
    "    # 1) Conditional VAE on indicators\n",
    "    print(\"1) Train Conditional VAE on indicators ...\")\n",
    "    vae = train_vae_on_indicators(df_raw, out_dir)\n",
    "\n",
    "    # 2) Latent features\n",
    "    print(\"2) Extract latent features ...\")\n",
    "    lat = vae_latent_df(vae, df_raw)\n",
    "\n",
    "    # 3) Merge back, + keep indicators as base for lags (as requested: use historical indicator values)\n",
    "    base = lat.merge(\n",
    "        df_raw[[\"UpazilaID\",\"year\",\"month\",\"ym\"] + INDICATORS + [target_col]],\n",
    "        on=[\"UpazilaID\",\"year\",\"month\",\"ym\"], how=\"left\", validate=\"1:1\"\n",
    "    )\n",
    "\n",
    "    # 4) Lags on indicators only (as per requirement)\n",
    "    print(\"3) Build lag features (1,3,6) + rolling stats ...\")\n",
    "    pc_cols = [c for c in base.columns if c.startswith(\"pc_mean_\") or c.startswith(\"pc_std_\")]\n",
    "    base, lag_cols = add_lags(base, INDICATORS, group_col=\"UpazilaID\", lags=(1,3,6), rolls=(3,6))\n",
    "\n",
    "    # 5) Feature set\n",
    "    feats = pc_cols + [\"month_sin\",\"month_cos\"] + lag_cols\n",
    "\n",
    "    # 6) Time split per Upazila\n",
    "    print(\"4) Split per Upazila: train / val (-12:-6) / test (-6:) ...\")\n",
    "    tr_parts, va_parts, te_parts = [], [], []\n",
    "    for uid, g in base.groupby(\"UpazilaID\"):\n",
    "        g = g.sort_values([\"year\",\"month\"]).copy()\n",
    "        if len(g) > (VAL_H_MONTHS + TEST_H_MONTHS):\n",
    "            te_parts.append(g.iloc[-TEST_H_MONTHS:])                                 # test\n",
    "            va_parts.append(g.iloc[-(VAL_H_MONTHS + TEST_H_MONTHS):-TEST_H_MONTHS])  # val\n",
    "            tr_parts.append(g.iloc[:-(VAL_H_MONTHS + TEST_H_MONTHS)])                # train\n",
    "        elif len(g) > TEST_H_MONTHS:\n",
    "            te_parts.append(g.iloc[-TEST_H_MONTHS:])\n",
    "            tr_parts.append(g.iloc[:-TEST_H_MONTHS])\n",
    "        else:\n",
    "            tr_parts.append(g)\n",
    "    tr_df = pd.concat(tr_parts).reset_index(drop=True)\n",
    "    va_df = pd.concat(va_parts).reset_index(drop=True) if va_parts else tr_df.iloc[0:0].copy()\n",
    "    te_df = pd.concat(te_parts).reset_index(drop=True) if te_parts else tr_df.iloc[0:0].copy()\n",
    "\n",
    "    # 7) Fit scalers on TRAIN\n",
    "    print(\"5) Fit scalers/encoders on TRAIN ...\")\n",
    "    split_tr, le_tr, fsc_tr, ysc_tr = build_mats(tr_df, feats, target_col, entity_col=\"UpazilaID\")\n",
    "\n",
    "    # Sequences\n",
    "    X_tr, Y_tr, C_tr = to_seq(split_tr, SEQ, True)\n",
    "    if X_tr.shape[0] == 0:\n",
    "        raise RuntimeError(\"No train sequences of length SEQ were formed. Reduce SEQ or check continuity.\")\n",
    "    print(f\"Train sequences: {X_tr.shape}\")\n",
    "\n",
    "    if len(va_df) > 0:\n",
    "        split_va = apply_mats(va_df, feats, target_col, le_tr, fsc_tr, ysc_tr, \"UpazilaID\")\n",
    "        X_va, Y_va, C_va = to_seq(split_va, SEQ, True)\n",
    "        if X_va.shape[0] == 0:\n",
    "            X_va, Y_va, C_va = X_tr, Y_tr, C_tr\n",
    "    else:\n",
    "        X_va, Y_va, C_va = X_tr, Y_tr, C_tr\n",
    "\n",
    "    if len(te_df) > 0:\n",
    "        split_te = apply_mats(te_df, feats, target_col, le_tr, fsc_tr, ysc_tr, \"UpazilaID\")\n",
    "        X_te, Y_te, C_te = to_seq(split_te, SEQ, True)\n",
    "        if X_te.shape[0] == 0:\n",
    "            X_te, Y_te, C_te = X_va, Y_va, C_va\n",
    "    else:\n",
    "        X_te, Y_te, C_te = X_va, Y_va, C_va\n",
    "\n",
    "    nc = int(C_tr.max()) + 1\n",
    "    cond_dim = X_tr.shape[2]\n",
    "    pc_idx = [i for i, f in enumerate(feats + [\"prev_y\"]) if f.startswith(\"pc_mean_\")]\n",
    "\n",
    "    # 8) Train GAN\n",
    "    print(\"6) Train forecasting GAN ...\")\n",
    "    G, D = train_gan(X_tr, Y_tr, C_tr, X_va, Y_va, C_va, cond_dim, nc, pc_idx, out_dir)\n",
    "\n",
    "    # 9) Evaluate\n",
    "    print(\"7) Evaluate on HELD-OUT TEST ...\")\n",
    "    overall, (mf, lo, hi, y) = evaluate(G, X_te, Y_te, C_te, ysc_tr, le_tr, label=\"HELD-OUT TEST\")\n",
    "\n",
    "    # save artifacts\n",
    "    np.savez(os.path.join(out_dir, \"predictions_test.npz\"), mf=mf, lo=lo, hi=hi, y=y, C_te=C_te)\n",
    "    json.dump({\n",
    "        \"metrics_test\": {k: float(v) for k,v in overall.items()},\n",
    "        \"config\": {\"seq_len\": SEQ, \"latent_dim\": LATENT, \"noise_dim\": NOISE, \"lstm_units\": LSTM_UNITS},\n",
    "        \"notes\": \"Upazila-wise split; last 6M TEST, prev 6M VAL; indicators→CondVAE→PCs + lagged indicators + seasonality.\"\n",
    "    }, open(os.path.join(out_dir,\"manifest.json\"),\"w\"), indent=2)\n",
    "\n",
    "    # simple scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.scatter(y, mf, alpha=0.6)\n",
    "    lim = float(max(y.max(), mf.max())*1.05)\n",
    "    ax.plot([0,lim],[0,lim],'r--')\n",
    "    ax.set_title(f'Actual vs Predicted — {target_col}')\n",
    "    ax.set_xlabel('Actual'); ax.set_ylabel('Pred')\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_dir,\"scatter_test.png\"), dpi=140); plt.close()\n",
    "    print(f\"[{target_col}] Artifacts saved to {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Load malaria data ...\")\n",
    "    df = load_malaria(MALARIA_CSV)\n",
    "\n",
    "    # ensure targets exist\n",
    "    need = {\"pv_rate\":\"PV/Population\",\"pf_rate\":\"PF/Population\",\"mixed_rate\":\"MIXED/Population\"}\n",
    "    for k,v in need.items():\n",
    "        if not np.isfinite(df[k]).any():\n",
    "            print(f\"[WARN] target {k} ({v}) appears empty or non-finite in your data.\")\n",
    "\n",
    "    # run three targets\n",
    "    targets = [(\"pv_rate\",\"Target_PV_over_Pop\"),\n",
    "               (\"pf_rate\",\"Target_PF_over_Pop\"),\n",
    "               (\"mixed_rate\",\"Target_MIXED_over_Pop\")]\n",
    "\n",
    "    for tgt, name in targets:\n",
    "        out_dir = os.path.join(OUT_ROOT, name); os.makedirs(out_dir, exist_ok=True)\n",
    "        # drop rows where target is missing at train time (we'll still preserve timeline continuity via to_seq)\n",
    "        df_t = df.copy()\n",
    "        # We keep rows for feature continuity but training matrices will reflect where label exists\n",
    "        run_for_target(df_t, tgt, out_dir)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1146dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
